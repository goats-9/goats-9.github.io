---
title: "CS3390 2023-08-25"
date: 2023-08-25T10:29:45Z
draft: false
toc: true
---

## Feature Selection

1. We can ideally use the \\(l\_0\\)-norm, which captures the number of nonzero elements of a vector, for feature selection.
2. It is not used since it is discontinuous. Thus, \\(l\_1\\)-norm is preferred for feature selection.

## Interpretation of Least Squares Method

### Geometric Interpretation
1. Consider the \\(M \times N\\) design matrix
\begin{equation}
\vec{\Phi} \triangleq \sbrak{\phi\_{j-1}\brak{\vec{x\_i}}}\_{ij}
\label{eq:Phi-def}
\end{equation}
2. The _basis functions_ \\(\phi\_j\brak{\vec{x}}\\) form a vector subspace, and the data vector \\(\vec{t}\\) has \\(N\\) dimensions.
3. The least-squares-regression function is obtained by finding the orthogonal projection of the data vector \\(\vec{t}\\) onto the subspace spanned by the basis functions.
4. The optimal weight vector is therefore
\begin{equation}
\vec{w^{\*}} \triangleq \brak{\vec{\Phi}^{\top}\vec{\Phi}}^{-1}\vec{\Phi}^{\top}\vec{t}
\label{eq:w-optimal}
\end{equation}
and consequently the orthogonal projection is 
\begin{equation}
\vec{\Phi w^{\*}} = \brak{\vec{\Phi}\brak{\vec{\Phi}^{\top}\vec{\Phi}}^{-1}\vec{\Phi}^{\top}}\vec{t}
\label{eq:orth-proj-t}
\end{equation}

### Probabilistic Interpretation: Maximum Likelihood Estimation

1. We write the output
\begin{equation}
y = \vec{w}^{\top}\vec{x} + \epsilon
\end{equation}
where the noise \\(\epsilon \sim N\brak{0, \sigma^2}\\). Thus, \\(y \sim N\brak{\vec{w}^{\top}\vec{x}, \sigma^2}\\). This is known as the _likelihood_ of \\(y\\).
2. Taking the log-likelihood function, we are required to find
\begin{align}
\log\brak{L} &= \argmax\_{\vec{w}}-\frac{1}{2\sigma\_n^2}\brak{y-\vec{w}^{\top}\vec{x}}^2 \\\\
&= \argmin\_{\vec{w}}\frac{1}{2}\brak{y-\vec{w}^{\top}\vec{x}}^2
\end{align}
which is the same as the least squares solution.

### MAP Estimate for Regularized Least Squares

1. Prior over parameters
\begin{equation}
p\brak{\vec{w}|\alpha} = \gauss{\vec{w}|\vec{0}}{\alpha^{-1}\vec{I}}
\label{eq:w-prior-over}
\end{equation}
2. Posteriori probability
\begin{equation}
p\brak{\vec{w}|\vec{x},\vec{t},\alpha,\beta} = p\brak{\vec{t}|\vec{x},\vec{w},\beta}p\brak{\vec{w}|\alpha}
\label{eq:posterior-est}
\end{equation}

## Bayesian Linear Regression
1. We define prediction as
\begin{equation}
p\brak{y|x} = \int p\brak{y|x,w}p\brak{w|D}dw
\label{eq:pred-def}
\end{equation}
2. The posterior, likelihood and prior probability is given by
\begin{equation}
p\brak{w|D} \propto p\brak{D|w}p\brak{w}
\label{eq:post-prior}
\end{equation}
3. These _ensemble_ methods give us multiple models, preventing _overfitting_ and providing for robust predictions.

## Cross Validation
1. This is used to choose an optimal value of \\(\lambda\\) in the regularized least squares regression problem.
2. We break a dataset into the following:
    - **Training Dataset**: Used to learn model parameters like \\(\vec{w}\\).
    - **Validation Dataset**: Set of data points that cannot be used for learning the model parameters but for tuning model hyperparameters such as \\(\lambda\\). Validation helps control overfitting.
    - **Test Dataset**: Assess performance of the final model and provide estimate of test error.
3. Split dataset randomly as 60 percent training, 20 percent each for validation and testing.
4. **Note**: dont use the test set to further tune the parameters or revise the model.
5. One can also split the dataset into _folds_ and find the optimal values considering the validation dataset to be different folds. The test dataset is separated beforehand.

## Training vs Generalization Error
1. Training error is not very useful as it can be made arbitrarily low.
2. Generalization error indicates how well the model will perform on unseen data.
