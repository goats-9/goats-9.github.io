---
title: "CS3390 2023-08-22"
date: 2023-08-22T09:01:05Z
draft: false
toc: true
---

## Polynomial Regression

### Curve Fitting

1.  Suppose we have to train a polynomial

    \begin{equation}
    y\brak{x,\vec{w}} = \sum_{j=0}^Mx\_jm^j.
    \label{eq:poly-eqn}
    \end{equation}

2.  Functions such as polynomials which are linear in the unknown parameters are
    called **linear models**.

3.  To minimize \eqref{eq:poly-eqn}, defining \\(\vec{X} \triangleq
    \myvec{\vec{x\_1} & \vec{x\_2} & \ldots & \vec{x\_N}}^{\top}\\), we get

    \begin{equation}
    \vec{w^{\*}} = \brak{\vec{XX}^{\top}}^{-1}\vec{Xy}
    \end{equation}

### Model Selection

1.  Model selection involves choosing the degree of the polynomial \\(M\\).

2.  Taking a high order polynomial provides very low training error but high
    test error. This is known as **overfitting**. It is _low_ bias, _high_
    variance.

3.  Choosing a small order polynomial provides very high training error and test
    error. This is known as **underfitting**. It is _high_ bias, _high_
    variance.

4.  Weight coefficients become larger as bias reduces.

5.  We want to choose the degree such that we are in between both of these
    regions.

6.  The overfitting problem becomes _less severe_ as the size of the dataset
    _increases_.

### Regularization

1.  Here, we try to reduce the magnitude of the weight coefficients to prevent
    bad performance on test data.

2.  Here, the error function is given by

    \begin{equation}
    \tilde{E}\brak{\vec{w}} = \frac{1}{2}\sum_{n=1}^N\brak{y\brak{x\_n,\vec{w}} - t\_n}^2 + \frac{\lambda}{2}\norm{\vec{w}}^2.
    \label{eq:err-reg}
    \end{equation}

3.  \\(\lambda\\) in \eqref{eq:err-reg} is known as the **regularization
    constant**.

4.  **Note**: for \\(p > 1\\), the \\(l\_p\\)-norm is given by

    \begin{equation}
    \norm{\vec{w}}\_{p} \triangleq \brak{\sum_{i=0}^{M}w\_i^p}^{\frac{1}{2}}.
    \label{eq:lp-norm-def}
    \end{equation}

    However, for \\(p = 1\\), the \\(l\_1\\)-norm is given by

    \begin{equation}
    \norm{\vec{w}}\_{1} \triangleq \sum_{i=0}^{M}\abs{w\_i}.
    \label{eq:l1-norm-def}
    \end{equation}

5. The coefficients learnt depend on the regularization constant.
    - Small constant implies large coefficients.
    - Large constant implies small coefficients.

### Regularized Least Squares

1.  For the \\(l\_q\\)-norm, the error function is given by

    \begin{equation}
    \tilde{E}\brak{\vec{w}} = \frac{1}{2}\sum_{n=1}^N\brak{y\brak{x\_n,\vec{w}} - t\_n}^2 + \frac{\lambda}{2}\norm{\vec{w}}^q.
    \label{eq:err-reg-gen}
    \end{equation}

2.  If \\(q=2\\), the method is known as **ridge regression**.
3.  If \\(q=1\\), the method is known as **lasso regression**. If \\(\lambda\\)
    is sufficiently large, some of the coefficients are driven to zero. One
    would prefer to use lasso regression in cases involving feature selection.
4.  One can also use _mixed-norm_ regularization or _elastic net_ regularization
5.  To solve for \\(\vec{w^{\*}}\\) in case of ridge regression, we have from
    \eqref{eq:err-reg},

    \begin{equation}
    \frac{\partial \tilde{E}\brak{\vec{w}}}{\partial \vec{w}} = -\vec{X}\brak{\vec{y}-\vec{X}^{\top}\vec{w}} + \lambda\vec{w}.
    \label{eq:partial-err-reg}
    \end{equation}

    Thus, setting the derivative to zero, we get

    \begin{equation}
    \vec{w^{\*}} = \brak{\vec{XX}^\top + \lambda\vec{I}}^{-1}\vec{Xy}.
    \label{eq:sol-ridge}
    \end{equation}

    Here \eqref{eq:sol-ridge} gives a stable solution, since we can find an
    inverse for some value of \\(\lambda\\).
