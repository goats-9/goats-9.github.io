---
title: "CS3390 2023-09-01"
date: 2023-09-01T10:59:28Z
draft: false
toc: true
---

## Logistic Regression

1. To classify, check the value of \\(k = \vec{w}^{\top}\vec{x}\\).
    - If \\(k > 0 \\), then \\(Y = 1\\) with greater probability, so we conclude that the object might belong to class 1.
    - If \\(k < 0\\), we conclude that the object belongs to class 0.
    - The set of \\(\vec{x}\\) for which \\(k = 0\\) is called the **decision boundary**. For an \\(d\\) dimensional input, the decision boundary is a \\(d-1\\) dimensional hyperplane.
2. The function
\begin{equation}
y\brak{\vec{x}} \triangleq \vec{w}^{\top}\vec{x}
\label{eq:discr-func}
\end{equation}
is known as a **discriminant function**. It need not be linear in \\(\vec{x}\\).

### Properties of Discriminant Function
1. \\(\vec{w}\\) is _orthogonal_ to all vectors lying on the decision surface.
2. The perpendicular distance of a point \\(\vec{x}\\) to the decision surface is given by
\begin{equation}
d\_{\vec{x}} \triangleq \frac{y\brak{\vec{x}}}{\norm{\vec{w}}}.
\label{eq:x-perp-decision}
\end{equation}
3. The perpendicular distance of the decision surface from the origin is
\begin{equation}
d\_{\vec{0}} = \frac{y\brak{\vec{0}}}{\norm{\vec{w}}}
\end{equation}

### Linear Regression for Logistic Regression
1. Linear regression not appropriate.
2. Distributes probability over continuous ranges.
3. Poisson regression better, distributes as a PMF.

