---
title: "CS3390 2023-09-05"
date: 2023-09-05T06:46:06Z
draft: false
toc: true
---

## Logistic Regression

1. We need to estimate, taking log likelihood,
\begin{equation}
\vec{w} = \argmax\_{\vec{w}}\sum\_i\ln\pr{y\_i|\vec{x\_i},\vec{w}} \\\\
\label{eq:w-log-reg}
\end{equation}

2. The log-likelihood on the right hand side of \eqref{eq:w-log-reg} may be written as
\begin{equation}
l\brak{\vec{w}} = \sum\_iy\_i\ln\pr{y\_i = 1|\vec{x\_i},\vec{w}} + \brak{1-y\_i}\ln\pr{y\_i = 0 | \vec{x\_i},\vec{w}}
\label{eq:log-reg-sum}
\end{equation}

3. We can maximize the negative of \eqref{eq:log-reg-sum} using gradient descent, since \\(f\brak{x} = \log\brak{x}\\) is concave.

4. Maximizing \eqref{eq:log-reg-sum} can lead to overfitting, especially for sparse high-dimensional data.

5. One approach is to penalize the log likelihood function as in regularization.

## Discriminant Analysis

1. We consider Linear Discriminant Analysis (LDA).
2. Here, \\(\vec{w}\\) is the direction to project \\(\vec{x}\\).
3. We are required to find \\(\vec{w}\\) such that when \\(\vec{x}\\) is projected, the classes are well separated.
