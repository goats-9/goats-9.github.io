---
title: "CS3390 2023-09-05"
date: 2023-09-05T06:46:06Z
draft: false
toc: true
---

## Logistic Regression

1. We need to estimate, taking log likelihood,
\begin{equation}
\vec{w} = \argmax\_{\vec{w}}\sum\_i\ln\pr{y\_i|\vec{x\_i},\vec{w}} \\\\
\label{eq:w-log-reg}
\end{equation}

2. The log-likelihood on the right hand side of \eqref{eq:w-log-reg} may be written as
\begin{equation}
l\brak{\vec{w}} = \sum\_iy\_i\ln\pr{y\_i = 1|\vec{x\_i},\vec{w}} + \brak{1-y\_i}\ln\pr{y\_i = 0 | \vec{x\_i},\vec{w}}
\label{eq:log-reg-sum}
\end{equation}

3. We can maximize the negative of \eqref{eq:log-reg-sum} using gradient descent, since \\(f\brak{x} = \log\brak{x}\\) is concave.

4. Maximizing \eqref{eq:log-reg-sum} can lead to overfitting, especially for sparse high-dimensional data.

5. One approach is to penalize the log likelihood function as in regularization.

## Discriminant Analysis

1. We consider Linear Discriminant Analysis (LDA).
2. Here, \\(\vec{w}\\) is the direction to project \\(\vec{x}\\).
3. We are required to find \\(\vec{w}\\) such that when \\(\vec{x}\\) is projected, the classes are well separated.
4. A simple measure is: Define
\begin{equation}
\vec{m\_i} = \frac{1}{N\_i}\sum\_{n\in\mathcal{C\_i}}\vec{x\_n}
\label{eq:mean-class}
\end{equation}
    We are now required to maximize (for binary classification)
    \begin{equation}
    m\_2 - m\_1 \triangleq \vec{w}^{\top}\brak{\vec{m\_2}-\vec{m\_1}}.
    \label{eq:mean-req}
    \end{equation}
5. We can make \eqref{eq:mean-req} arbitratily large by increasing the magnitude of \\(\vec{w}\\). Hence, we introduce the following constraints.
    1. Constrain \\(\vec{w}\\) to unit length.
    2. Use Lagrange multipliers for the constrained maximization.
    3. Find the solution \\(\vec{w} \propto \brak{\vec{m\_2}-\vec{m\_1}}\\).

### Fisher's Linear Discriminant

1. Idea is to maximize a function that will give a large separation between the prohected class menas while giving a small variance within each class, thereby minimizing class overlap.
2. We have between-class and within-class measures of scatter.
3. We need to find \\(\vec{w}\\) that maximizes
\begin{equation}
J\brak{\vec{w}} = \frac{\brak{m\_1-m\_2}^2}{s\_1^2 + s\_2^2}
\end{equation}
where
\begin{align}
m\_1 &\triangleq \frac{\sum\_{t}\vec{w}^{\top}{\vec{x\_t}r\_t}}{\sum\_{t}r\_t} \\\\
s\_1^2 &\triangleq \sum\_{t}\brak{\vec{w}^{\top}\vec{x\_t} - m\_1}^2r\_t
\end{align}

### Lagrange Multipliers

1. These are used to solve constrained problems of the form
\begin{align}
    \max\ &f\brak{\vec{x}} \nonumber \\\\
    \text{s.t. } &g\brak{\vec{x}} = 0
\end{align}

2. At the optimal point, we must have
\begin{equation}
\nabla f\brak{\vec{x}} = \lambda\nabla g\brak{\vec{x}}
\label{eq:lag-cond}
\end{equation}
that is, the gradients of both functions are perpendicular to the constrained surface.

3. Note that \eqref{eq:lag-cond} is necessaary but not sufficient.

4. Thus, we require
\begin{equation}
\argmax\_{\vec{x},\lambda}L\brak{\vec{x},\lambda}
\end{equation}
where
\begin{equation}
L\brak{\vec{x},\lambda} \triangleq f\brak{\vec{x}} + \lambda g\brak{\vec{x}}.
\label{eq:lag-func}
\end{equation}

5. For the LDA approach,
\begin{equation}
L\brak{\vec{x},\lambda} = \vec{w}^{\top}\brak{\vec{m\_2}-\vec{m\_1}} + \lambda\brak{\vec{w}^{\top}\vec{w}-1}.
\label{eq:lda-lag}
\end{equation}

6. Taking the gradient of \eqref{eq:lda-lag} and setting to zero gives
\begin{equation}
\vec{w^{\*}} = \frac{1}{2\lambda}\brak{\vec{m\_1}-\vec{m\_2}}
\label{eq:w-opt-lag}
\end{equation}
