---
title: "CS3390 2023-08-29"
date: 2023-08-29T09:01:33Z
draft: false
toc: true
---

## Supervised Learning: Regression

### Poisson Regression

1. The Poisson distribution is given by
\begin{equation}
\pr{k \textrm{ events in interval}} = e^{-\lambda}\frac{\lambda^k}{k!}
\label{eq:poisson-pmf}
\end{equation}
2. Here, \\(\lambda\\) is the mean number of events. Since it is nonnegative, we have
\begin{equation}
\lambda = \exp\brak{\vec{w}^{\top}\vec{x}}
\label{eq:lambda-model}
\end{equation}
3. The likelihood is given by
\begin{equation}
p\brak{y\_1,,\ldots,y\_m|x\_1,\ldots,x\_m;\theta} = \prod_{i=1}^m\frac{\exp\brak{y\_i\brak{\vec{w}^{\top}\vec{x}} - \exp\brak{\vec{w}^{\top}\vec{x}}}}{y\_i!}
\label{eq:poisson-like}
\end{equation}
4. We can maximize \eqref{eq:poisson-like} using log-likelihood and gradient descent.

## Bias-Variance Decomposition

1. Given the actual distributions \\(p\brak{y|\vec{x}}\\) and \\(p\brak{\vec{x},y}\\), let the optimal function that can be learnt be \\(h\brak{\vec{x}}\\).
2. For a given dataset \\(\mathcal{D}\\), let the prediction function obtained be \\(y\brak{\vec{x};\mathcal{D}}\\).
3. The expected squared difference between the two is
\begin{align}
&\mathbb{E}\_{\mathcal{D}}\sbrak{\cbrak{y\brak{\vec{x};\mathcal{D}}-h\brak{\vec{x}}}^2} \nonumber \\\\
&= \cbrak{\mathbb{E}\_{\mathcal{D}}\brak{y\brak{\vec{x};\mathcal{D}}} - h\brak{\vec{x}}}^2 + \mathbb{E}\_{\mathcal{D}}\sbrak{\cbrak{y\brak{\vec{x};\mathcal{D}} - \mathbb{E}\_{\mathcal{D}}\sbrak{y\brak{\vec{x};\mathcal{D}}}}^2}
\label{eq:bias-var}
\end{align}

4. In \eqref{eq:bias-var}, the first term is the square of the bias, and the second term is known as the variance.
5. Bias Error arises from the simplyfiying assumptions made by a model to make the target easier to learn.
6. Variance is the amount of estimate of the target function will change if different training data is used.
7. **Linear models**: high bias, low variance. Non-linear is low bias, high variance.

## Bias-Variance Tradeoff
1. Dimensionality reduction and feature selection can decrease variance by simplifying models.
2. Larger training set tends to decrease variance.
3. Adding features tends to decrease bias at the expense of increasing variance.
4. Linear and generalized linear models can be regularized to decrease variance at the cost of increasing their bias.

## Supervised Learning: Classification

1. For a binary classification problem, we model
\begin{equation}
\pr{Y|X} \sim \textrm{Ber}\brak{p}
\end{equation}
2. We model \\(p\\) using a **sigmoid** function. Examples:
    - _Logistic sigmoid_ function: \\(f\brak{x} = \frac{1}{1+e^{-x}}\\)
    - CDFs of probability distributions. In particular, for a normal random vairable, \\(F\brak{x} = \int\_{0}^xN\brak{z;0,1}dz\\)
3. When the input \\(\vec{x}\\) is a vector, we consider a linear transformation and model \\(p  = g\brak{\vec{w}^{\top}\vec{x}}\\), where \\(g\\) is a sigmoid function.
4. Thus, for logistic regression, we write
\begin{equation}
\pr{Y=1|\vec{x},\vec{w}} = \frac{1}{1 + e^{-\brak{\vec{w}^{\top}\vec{x}}}}.
\label{eq:p-yx-log}
\end{equation}
