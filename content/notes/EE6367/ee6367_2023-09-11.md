---
title: "EE6367 2023-09-11"
date: 2023-09-11T04:38:47Z
draft: false
toc: true
---

## Cramer-Rao Bound

Suppose \\(f\_{X^n|\theta}\\) is a known density and \\(g\brak{\hat{X}}\\) is any estimator such that

\begin{equation}
    \frac{\partial}{\partial\theta}\int_{x^n\in\mathbb{R}^n}f\_{X^n|\theta}\brak{x^n}g\brak{x^n}dx^n = \int_{x^n\in\mathbb{R}^n}g\brak{x^n}\brak{\frac{\partial f\_{X^n|\theta}\brak{x^n}}{\partial\theta}}dx^n
\end{equation}

then,

\begin{equation}
    \textrm{Var}\brak{g\brak{X^n}} \ge \frac{\brak{\frac{\partial}{\partial\theta}\mean{g\brak{X^n}}}^2}{\mean{\brak{\frac{\partial}{\partial\theta}\log f\_{X^n|\theta}\brak{X^n}}^2}}
\end{equation}

For an unbiased estimator, \\(\mean{g\brak{X^n}} = \theta\\) and \\(\textrm{MSE}\brak{g} = \textrm{Var}\brak{g\brak{X^n}}\\). Thus, the equality reads

\begin{equation}
    \textrm{MSE}\brak{g} \ge \frac{1}{\mean{\brak{\frac{\partial}{\partial\theta}\log f\_{X^n|\theta}\brak{X^n}}^2}}.
\end{equation}

The denominator is called the **Fisher information**.

We prove this equality using the Cacuhy-Schwarz inequality

\begin{equation}
    \brak{\textrm{Cov}\brak{U,V}}^2 \le \textrm{Var}\brak{U}\textrm{Var}\brak{V}.
\end{equation}

Taking \\(U = g\brak{X^n}\\) and \\(V=\frac{\partial}{\partial\theta}\log{f\_{X^n|\theta}\brak{X^n}}\\), and noting that

\begin{align}
    \mean{V} &= \int_{\mathbb{R}^n}f\_{X^n|\theta}\brak{x^n}\frac{\partial}{\partial\theta}\log{f\_{X^n|\theta}\brak{x^n}}dx^n \\\\
    &= \frac{\partial}{\partial\theta}\int_{\mathbb{R}^n}f\_{X^n|\theta}\brak{x^n}dx^n = 0
\end{align}

Hence,

\begin{align}
    \textrm{Cov}\brak{U,V} &= \mean{\brak{U-\mean{U}}\brak{V-\mean{V}}} \\\\
    &= \mean{UV} - \mean{U}\mean{V} \\\\
    &= \mean{UV} \\\\
    &= \int_{\mathbb{R}^n}g\brak{x^n}\frac{\partial}{\partial\theta}f\_{X^n|\theta}\brak{x^n}dx^n \\\\
    &= \frac{\partial}{\partial\theta}\mean{g\brak{X^n}}
\end{align}

For example, suppose that \\(X\_i,\ 1 \le i \le n \sim \gauss{\theta}{\sigma^2}\\) are iid. Then,

\begin{align}
    \frac{\partial}{\partial\theta}\log\sbrak{f\_{X^n|\theta}\brak{x^n}} &= \frac{\partial}{\partial\theta} \sum\_{i=1}^n\sbrak{-\frac{\brak{x\_i-\theta}^2}{2\sigma^2}-\frac{1}{2}\log\brak{2\pi\sigma^2}} \\\\
    &= \sum\_{i=1}^n\frac{X\_i-\theta}{\sigma^2}.
\end{align}

Thus, the Fisher information is given by

\begin{equation}
\mean{\brak{\sum\_{i=1}^n\frac{X\_i-\theta}{\sigma^2}}^2} = \frac{n}{\sigma^2}
\end{equation}

## Estimation Protocols

Consider a case where the iid samples are distributed across various locations. The iid samples need to be sent to a central server to output a mean. We are now bound by communication constraints to send information to a server. We now need to replace estimators by _estimation protocols_.

Some use cases include

1. Sensor networks
2. Federated learning

## References

1. _Statistical Inference_, Casella & Berger.
2. _All of statistics_, Larry Wasserman.
3. _An Introduction to Signal Detection and Estimation_, H. V. Poor.
